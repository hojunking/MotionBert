{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import errno\n",
    "import math\n",
    "import pickle\n",
    "import tensorboardX\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import copy\n",
    "import random\n",
    "import prettytable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from lib.utils.tools import *\n",
    "from lib.utils.learning import *\n",
    "from lib.utils.utils_data import flip_data\n",
    "from lib.data.dataset_motion_2d import PoseTrackDataset2D, InstaVDataset2D\n",
    "from lib.data.dataset_motion_3d import MotionDataset3D\n",
    "from lib.data.augmentation import Augmenter2D\n",
    "from lib.data.datareader_h36m import DataReaderH36M  \n",
    "from lib.model.loss import *\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/pretrain.yaml\", help=\"Path to the config file.\")\n",
    "    parser.add_argument('-c', '--checkpoint', default='checkpoint', type=str, metavar='PATH', help='checkpoint directory')\n",
    "    parser.add_argument('-p', '--pretrained', default='checkpoint', type=str, metavar='PATH', help='pretrained checkpoint directory')\n",
    "    parser.add_argument('-r', '--resume', default='', type=str, metavar='FILENAME', help='checkpoint to resume (file name)')\n",
    "    parser.add_argument('-e', '--evaluate', default='', type=str, metavar='FILENAME', help='checkpoint to evaluate (file name)')\n",
    "    parser.add_argument('-ms', '--selection', default='latest_epoch.bin', type=str, metavar='FILENAME', help='checkpoint to finetune (file name)')\n",
    "    parser.add_argument('-sd', '--seed', default=0, type=int, help='random seed')\n",
    "    opts = parser.parse_args()\n",
    "    return opts\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def save_checkpoint(chk_path, epoch, lr, optimizer, model_pos, min_loss):\n",
    "    print('Saving checkpoint to', chk_path)\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'lr': lr,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_pos': model_pos.state_dict(),\n",
    "        'min_loss' : min_loss\n",
    "    }, chk_path)\n",
    "    \n",
    "def evaluate(args, model_pos, test_loader, datareader):\n",
    "    print('INFO: Testing')\n",
    "    results_all = []\n",
    "    model_pos.eval()            \n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_gt in tqdm(test_loader):\n",
    "            N, T = batch_gt.shape[:2]\n",
    "            if torch.cuda.is_available():\n",
    "                batch_input = batch_input.cuda()\n",
    "            if args.no_conf:\n",
    "                batch_input = batch_input[:, :, :, :2]\n",
    "            if args.flip:    \n",
    "                batch_input_flip = flip_data(batch_input)\n",
    "                predicted_3d_pos_1 = model_pos(batch_input)\n",
    "                predicted_3d_pos_flip = model_pos(batch_input_flip)\n",
    "                predicted_3d_pos_2 = flip_data(predicted_3d_pos_flip)                   # Flip back\n",
    "                predicted_3d_pos = (predicted_3d_pos_1+predicted_3d_pos_2) / 2\n",
    "            else:\n",
    "                predicted_3d_pos = model_pos(batch_input)\n",
    "            if args.rootrel:\n",
    "                predicted_3d_pos[:,:,0,:] = 0     # [N,T,17,3]\n",
    "            else:\n",
    "                batch_gt[:,0,0,2] = 0\n",
    "\n",
    "            if args.gt_2d:\n",
    "                predicted_3d_pos[...,:2] = batch_input[...,:2]\n",
    "            results_all.append(predicted_3d_pos.cpu().numpy())\n",
    "    results_all = np.concatenate(results_all)\n",
    "    results_all = datareader.denormalize(results_all)\n",
    "    _, split_id_test = datareader.get_split_id()\n",
    "    actions = np.array(datareader.dt_dataset['test']['action'])\n",
    "    factors = np.array(datareader.dt_dataset['test']['2.5d_factor'])\n",
    "    gts = np.array(datareader.dt_dataset['test']['joints_2.5d_image'])\n",
    "    sources = np.array(datareader.dt_dataset['test']['source'])\n",
    "\n",
    "    num_test_frames = len(actions)\n",
    "    frames = np.array(range(num_test_frames))\n",
    "    action_clips = actions[split_id_test]\n",
    "    factor_clips = factors[split_id_test]\n",
    "    source_clips = sources[split_id_test]\n",
    "    frame_clips = frames[split_id_test]\n",
    "    gt_clips = gts[split_id_test]\n",
    "    assert len(results_all)==len(action_clips)\n",
    "    \n",
    "    e1_all = np.zeros(num_test_frames)\n",
    "    e2_all = np.zeros(num_test_frames)\n",
    "    oc = np.zeros(num_test_frames)\n",
    "    results = {}\n",
    "    results_procrustes = {}\n",
    "    action_names = sorted(set(datareader.dt_dataset['test']['action']))\n",
    "    for action in action_names:\n",
    "        results[action] = []\n",
    "        results_procrustes[action] = []\n",
    "    block_list = ['s_09_act_05_subact_02', \n",
    "                  's_09_act_10_subact_02', \n",
    "                  's_09_act_13_subact_01']\n",
    "    for idx in range(len(action_clips)):\n",
    "        source = source_clips[idx][0][:-6]\n",
    "        if source in block_list:\n",
    "            continue\n",
    "        frame_list = frame_clips[idx]\n",
    "        action = action_clips[idx][0]\n",
    "        factor = factor_clips[idx][:,None,None]\n",
    "        gt = gt_clips[idx]\n",
    "        pred = results_all[idx]\n",
    "        pred *= factor\n",
    "        \n",
    "        # Root-relative Errors\n",
    "        pred = pred - pred[:,0:1,:]\n",
    "        gt = gt - gt[:,0:1,:]\n",
    "        err1 = mpjpe(pred, gt)\n",
    "        err2 = p_mpjpe(pred, gt)\n",
    "        e1_all[frame_list] += err1\n",
    "        e2_all[frame_list] += err2\n",
    "        oc[frame_list] += 1\n",
    "    for idx in range(num_test_frames):\n",
    "        if e1_all[idx] > 0:\n",
    "            err1 = e1_all[idx] / oc[idx]\n",
    "            err2 = e2_all[idx] / oc[idx]\n",
    "            action = actions[idx]\n",
    "            results[action].append(err1)\n",
    "            results_procrustes[action].append(err2)\n",
    "    final_result = []\n",
    "    final_result_procrustes = []\n",
    "    summary_table = prettytable.PrettyTable()\n",
    "    summary_table.field_names = ['test_name'] + action_names\n",
    "    for action in action_names:\n",
    "        final_result.append(np.mean(results[action]))\n",
    "        final_result_procrustes.append(np.mean(results_procrustes[action]))\n",
    "    summary_table.add_row(['P1'] + final_result)\n",
    "    summary_table.add_row(['P2'] + final_result_procrustes)\n",
    "    print(summary_table)\n",
    "    e1 = np.mean(np.array(final_result))\n",
    "    e2 = np.mean(np.array(final_result_procrustes))\n",
    "    print('Protocol #1 Error (MPJPE):', e1, 'mm')\n",
    "    print('Protocol #2 Error (P-MPJPE):', e2, 'mm')\n",
    "    print('----------')\n",
    "    return e1, e2, results_all\n",
    "        \n",
    "def train_epoch(args, model_pos, train_loader, losses, optimizer, has_3d, has_gt):\n",
    "    model_pos.train()\n",
    "    \n",
    "    for idx, (batch_input, batch_gt) in tqdm(enumerate(train_loader)):    \n",
    "        batch_size = len(batch_input)        \n",
    "        if torch.cuda.is_available():\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_gt = batch_gt.cuda()\n",
    "        with torch.no_grad():\n",
    "            if args.no_conf:\n",
    "                batch_input = batch_input[:, :, :, :2]\n",
    "            if not has_3d:\n",
    "                conf = copy.deepcopy(batch_input[:,:,:,2:])    # For 2D data, weight/confidence is at the last channel\n",
    "            if args.rootrel:\n",
    "                batch_gt = batch_gt - batch_gt[:,:,0:1,:]\n",
    "            else:\n",
    "                batch_gt[:,:,:,2] = batch_gt[:,:,:,2] - batch_gt[:,0:1,0:1,2] # Place the depth of first frame root to 0.\n",
    "            if args.mask or args.noise:\n",
    "                batch_input = args.aug.augment2D(batch_input, noise=(args.noise and has_gt), mask=args.mask)\n",
    "        # Predict 3D poses\n",
    "        predicted_3d_pos = model_pos(batch_input)    # (N, T, 17, 3)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if has_3d:\n",
    "            loss_3d_pos = loss_mpjpe(predicted_3d_pos, batch_gt)\n",
    "            loss_3d_scale = n_mpjpe(predicted_3d_pos, batch_gt)\n",
    "            loss_3d_velocity = loss_velocity(predicted_3d_pos, batch_gt)\n",
    "            loss_lv = loss_limb_var(predicted_3d_pos)\n",
    "            loss_lg = loss_limb_gt(predicted_3d_pos, batch_gt)\n",
    "            loss_a = loss_angle(predicted_3d_pos, batch_gt)\n",
    "            loss_av = loss_angle_velocity(predicted_3d_pos, batch_gt)\n",
    "            loss_total = loss_3d_pos + \\\n",
    "                         args.lambda_scale       * loss_3d_scale + \\\n",
    "                         args.lambda_3d_velocity * loss_3d_velocity + \\\n",
    "                         args.lambda_lv          * loss_lv + \\\n",
    "                         args.lambda_lg          * loss_lg + \\\n",
    "                         args.lambda_a           * loss_a  + \\\n",
    "                         args.lambda_av          * loss_av\n",
    "            losses['3d_pos'].update(loss_3d_pos.item(), batch_size)\n",
    "            losses['3d_scale'].update(loss_3d_scale.item(), batch_size)\n",
    "            losses['3d_velocity'].update(loss_3d_velocity.item(), batch_size)\n",
    "            losses['lv'].update(loss_lv.item(), batch_size)\n",
    "            losses['lg'].update(loss_lg.item(), batch_size)\n",
    "            losses['angle'].update(loss_a.item(), batch_size)\n",
    "            losses['angle_velocity'].update(loss_av.item(), batch_size)\n",
    "            losses['total'].update(loss_total.item(), batch_size)\n",
    "        else:\n",
    "            loss_2d_proj = loss_2d_weighted(predicted_3d_pos, batch_gt, conf)\n",
    "            loss_total = loss_2d_proj\n",
    "            losses['2d_proj'].update(loss_2d_proj.item(), batch_size)\n",
    "            losses['total'].update(loss_total.item(), batch_size)\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def train_with_config(args, opts):\n",
    "    print(args)\n",
    "    try:\n",
    "        os.makedirs(opts.checkpoint)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise RuntimeError('Unable to create checkpoint directory:', opts.checkpoint)\n",
    "    train_writer = tensorboardX.SummaryWriter(os.path.join(opts.checkpoint, \"logs\"))\n",
    "\n",
    "\n",
    "    print('Loading dataset...')\n",
    "    trainloader_params = {\n",
    "          'batch_size': args.batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 12,\n",
    "          'pin_memory': True,\n",
    "          'prefetch_factor': 4,\n",
    "          'persistent_workers': True\n",
    "    }\n",
    "    \n",
    "    testloader_params = {\n",
    "          'batch_size': args.batch_size,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 12,\n",
    "          'pin_memory': True,\n",
    "          'prefetch_factor': 4,\n",
    "          'persistent_workers': True\n",
    "    }\n",
    "\n",
    "    train_dataset = MotionDataset3D(args, args.subset_list, 'train')\n",
    "    test_dataset = MotionDataset3D(args, args.subset_list, 'test')\n",
    "    train_loader_3d = DataLoader(train_dataset, **trainloader_params)\n",
    "    test_loader = DataLoader(test_dataset, **testloader_params)\n",
    "    \n",
    "    if args.train_2d:\n",
    "        posetrack = PoseTrackDataset2D()\n",
    "        posetrack_loader_2d = DataLoader(posetrack, **trainloader_params)\n",
    "        instav = InstaVDataset2D()\n",
    "        instav_loader_2d = DataLoader(instav, **trainloader_params)\n",
    "        \n",
    "    datareader = DataReaderH36M(n_frames=args.clip_len, sample_stride=args.sample_stride, data_stride_train=args.data_stride, data_stride_test=args.clip_len, dt_root = 'data/motion3d', dt_file=args.dt_file)\n",
    "    min_loss = 100000\n",
    "    model_backbone = load_backbone(args)\n",
    "    model_params = 0\n",
    "    for parameter in model_backbone.parameters():\n",
    "        model_params = model_params + parameter.numel()\n",
    "    print('INFO: Trainable parameter count:', model_params)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model_backbone = nn.DataParallel(model_backbone)\n",
    "        model_backbone = model_backbone.cuda()\n",
    "\n",
    "    if args.finetune:\n",
    "        if opts.resume or opts.evaluate:\n",
    "            chk_filename = opts.evaluate if opts.evaluate else opts.resume\n",
    "            print('Loading checkpoint', chk_filename)\n",
    "            checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
    "            model_backbone.load_state_dict(checkpoint['model_pos'], strict=True)\n",
    "            model_pos = model_backbone\n",
    "        else:\n",
    "            chk_filename = os.path.join(opts.pretrained, opts.selection)\n",
    "            print('Loading checkpoint', chk_filename)\n",
    "            checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
    "            model_backbone.load_state_dict(checkpoint['model_pos'], strict=True)\n",
    "            model_pos = model_backbone            \n",
    "    else:\n",
    "        chk_filename = os.path.join(opts.checkpoint, \"latest_epoch.bin\")\n",
    "        if os.path.exists(chk_filename):\n",
    "            opts.resume = chk_filename\n",
    "        if opts.resume or opts.evaluate:\n",
    "            chk_filename = opts.evaluate if opts.evaluate else opts.resume\n",
    "            print('Loading checkpoint', chk_filename)\n",
    "            checkpoint = torch.load(chk_filename, map_location=lambda storage, loc: storage)\n",
    "            model_backbone.load_state_dict(checkpoint['model_pos'], strict=True)\n",
    "        model_pos = model_backbone\n",
    "        \n",
    "    if args.partial_train:\n",
    "        model_pos = partial_train_layers(model_pos, args.partial_train)\n",
    "\n",
    "    if not opts.evaluate:        \n",
    "        lr = args.learning_rate\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_pos.parameters()), lr=lr, weight_decay=args.weight_decay)\n",
    "        lr_decay = args.lr_decay\n",
    "        st = 0\n",
    "        if args.train_2d:\n",
    "            print('INFO: Training on {}(3D)+{}(2D) batches'.format(len(train_loader_3d), len(instav_loader_2d) + len(posetrack_loader_2d)))\n",
    "        else:\n",
    "            print('INFO: Training on {}(3D) batches'.format(len(train_loader_3d)))\n",
    "        if opts.resume:\n",
    "            st = checkpoint['epoch']\n",
    "            if 'optimizer' in checkpoint and checkpoint['optimizer'] is not None:\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            else:\n",
    "                print('WARNING: this checkpoint does not contain an optimizer state. The optimizer will be reinitialized.')            \n",
    "            lr = checkpoint['lr']\n",
    "            if 'min_loss' in checkpoint and checkpoint['min_loss'] is not None:\n",
    "                min_loss = checkpoint['min_loss']\n",
    "                \n",
    "        args.mask = (args.mask_ratio > 0 and args.mask_T_ratio > 0)\n",
    "        if args.mask or args.noise:\n",
    "            args.aug = Augmenter2D(args)\n",
    "        \n",
    "        # Training\n",
    "        for epoch in range(st, args.epochs):\n",
    "            print('Training epoch %d.' % epoch)\n",
    "            start_time = time()\n",
    "            losses = {}\n",
    "            losses['3d_pos'] = AverageMeter()\n",
    "            losses['3d_scale'] = AverageMeter()\n",
    "            losses['2d_proj'] = AverageMeter()\n",
    "            losses['lg'] = AverageMeter()\n",
    "            losses['lv'] = AverageMeter()\n",
    "            losses['total'] = AverageMeter()\n",
    "            losses['3d_velocity'] = AverageMeter()\n",
    "            losses['angle'] = AverageMeter()\n",
    "            losses['angle_velocity'] = AverageMeter()\n",
    "            N = 0\n",
    "                        \n",
    "            # Curriculum Learning\n",
    "            if args.train_2d and (epoch >= args.pretrain_3d_curriculum):\n",
    "                train_epoch(args, model_pos, posetrack_loader_2d, losses, optimizer, has_3d=False, has_gt=True)\n",
    "                train_epoch(args, model_pos, instav_loader_2d, losses, optimizer, has_3d=False, has_gt=False)\n",
    "            train_epoch(args, model_pos, train_loader_3d, losses, optimizer, has_3d=True, has_gt=True) \n",
    "            elapsed = (time() - start_time) / 60\n",
    "\n",
    "            if args.no_eval:\n",
    "                print('[%d] time %.2f lr %f 3d_train %f' % (\n",
    "                    epoch + 1,\n",
    "                    elapsed,\n",
    "                    lr,\n",
    "                   losses['3d_pos'].avg))\n",
    "            else:\n",
    "                e1, e2, results_all = evaluate(args, model_pos, test_loader, datareader)\n",
    "                print('[%d] time %.2f lr %f 3d_train %f e1 %f e2 %f' % (\n",
    "                    epoch + 1,\n",
    "                    elapsed,\n",
    "                    lr,\n",
    "                    losses['3d_pos'].avg,\n",
    "                    e1, e2))\n",
    "                train_writer.add_scalar('Error P1', e1, epoch + 1)\n",
    "                train_writer.add_scalar('Error P2', e2, epoch + 1)\n",
    "                train_writer.add_scalar('loss_3d_pos', losses['3d_pos'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_2d_proj', losses['2d_proj'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_3d_scale', losses['3d_scale'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_3d_velocity', losses['3d_velocity'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_lv', losses['lv'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_lg', losses['lg'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_a', losses['angle'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_av', losses['angle_velocity'].avg, epoch + 1)\n",
    "                train_writer.add_scalar('loss_total', losses['total'].avg, epoch + 1)\n",
    "                \n",
    "            # Decay learning rate exponentially\n",
    "            lr *= lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= lr_decay\n",
    "\n",
    "            # Save checkpoints\n",
    "            chk_path = os.path.join(opts.checkpoint, 'epoch_{}.bin'.format(epoch))\n",
    "            chk_path_latest = os.path.join(opts.checkpoint, 'latest_epoch.bin')\n",
    "            chk_path_best = os.path.join(opts.checkpoint, 'best_epoch.bin'.format(epoch))\n",
    "            \n",
    "            save_checkpoint(chk_path_latest, epoch, lr, optimizer, model_pos, min_loss)\n",
    "            if (epoch + 1) % args.checkpoint_frequency == 0:\n",
    "                save_checkpoint(chk_path, epoch, lr, optimizer, model_pos, min_loss)\n",
    "            if e1 < min_loss:\n",
    "                min_loss = e1\n",
    "                save_checkpoint(chk_path_best, epoch, lr, optimizer, model_pos, min_loss)\n",
    "                \n",
    "    if opts.evaluate:\n",
    "        e1, e2, results_all = evaluate(args, model_pos, test_loader, datareader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opts = parse_args()\n",
    "    set_random_seed(opts.seed)\n",
    "    args = get_config(opts.config)\n",
    "    train_with_config(args, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
